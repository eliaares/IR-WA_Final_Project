{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final project IR-WA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ãˆlia Armengol 205744 Gabriela Cid 206085"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "import collections\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "from config import *\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import string\n",
    "import unidecode\n",
    "import re\n",
    "from array import array\n",
    "from numpy import linalg as la\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "# Set Pandas to show all the columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from tweepy import API\n",
    "from tweepy import Cursor\n",
    "import json\n",
    "import datetime\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to recollect information from the tweets with the theme Black Lives Matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download tweepy\n",
    "\n",
    "## access token informations \n",
    "access_token1 = \"2801892334-ahdScaxkY1zLMOX4WpKJRHin470YU0RG7cA7Zxo\"\n",
    "access_token_secret1 = \"gktWBlIyJYDaN0R3ST283iYd9coDbZPT505nSZHcmlLmN\"\n",
    "\n",
    "consumer_key1 = \"JdxyZOEGkF6IHW5wykjSNd8or\"\n",
    "consumer_secret1 = \"43cHQOySq5bzWXV3X7HNjdBi4lSQbi37UOgEAnnj2U2AHttL8R\"\n",
    "bearer = \"AAAAAAAAAAAAAAAAAAAAABq5KQEAAAAAmxWRR2sLSoQ%2BKL2L14R4fEDySVI%3Dy1o9dfDqaPzi70HSpYTZjMEoaaPOMjxVWxtXpulZ5z8j1buVr8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = OAuthHandler(consumer_key1, consumer_secret1)\n",
    "auth.set_access_token(access_token1, access_token_secret1)\n",
    "api = API(auth_handler=auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyStreamListener(StreamListener):\n",
    "    \"\"\"\n",
    "    Twitter listener, collects streaming tweets and output to a file\n",
    "    \"\"\"\n",
    "    def __init__(self, api, OUTPUT_FILENAME, stop_condition=10):\n",
    "        \"\"\"\n",
    "        initialize the stream, with num. of tweets and saving the outputfile\n",
    "        \"\"\"\n",
    "        \n",
    "        # this line is needed to import the characteristics of the streaming API\n",
    "        super(MyStreamListener, self).__init__()\n",
    "        \n",
    "        # to-count the number of tweets collected        \n",
    "        self.num_tweets = 0\n",
    "        \n",
    "        # save filename\n",
    "        self.filename = OUTPUT_FILENAME\n",
    "        \n",
    "        # stop-condition\n",
    "        self.stop_condition = stop_condition\n",
    "        \n",
    "\n",
    "    def on_status(self, status):\n",
    "        \n",
    "        \"\"\"\n",
    "        this function runs each time a new bunch of tweets is retrived from the streaming\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(self.filename, \"a+\") as f:\n",
    "            tweet = status._json\n",
    "            \n",
    "            f.write(json.dumps(tweet) + '\\n')\n",
    "            #self.output.append(tweet)\n",
    "            self.num_tweets += 1\n",
    "        \n",
    "            # Stop condition        \n",
    "            if self.num_tweets <= self.stop_condition:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        \n",
    "\n",
    "    def on_error(self, status):\n",
    "        \"\"\"\n",
    "        function useful to handle errors. It's possible to personalize it \n",
    "        depending on the way we want to handle errors\n",
    "        \"\"\"\n",
    "        \n",
    "        print(status)\n",
    "        #returning False in on_error disconnects the stream\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "OUTPUT_FILENAME = \"data/one.json\"\n",
    "stop_condition = 1500\n",
    "\n",
    "l = MyStreamListener(api, OUTPUT_FILENAME, stop_condition)\n",
    "# here we recall the Stream Class from Tweepy to input the authentication info and our personalized listener \n",
    "stream = Stream(auth=api.auth, listener=l)\n",
    "\n",
    "\n",
    "# keywords we may want decide to track \n",
    "TRACKING_KEYWORDS = [\"#BLM\", \"#BlackLivesMatter\", \"#BreonnaTaylor\", \"#GeorgeFloyd\", \"black lives\", \"black\", \"black people\"]\n",
    "stream.filter(\n",
    "    track=TRACKING_KEYWORDS, \n",
    "    is_async=False, \n",
    "    languages = [\"en\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Build the Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/one.json\", \"rb\") as f:\n",
    "    data = f.readlines()\n",
    "    data = [json.loads(str_) for str_ in data]\n",
    "\n",
    "df_tweets = pd.DataFrame.from_records(data)\n",
    "\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = df_tweets[df_tweets['extended_tweet'].notna()]\n",
    "df_tweets = df_tweets.reset_index(drop=True)\n",
    "df_tweets = df_tweets.drop(['id', 'id_str'], axis=1)\n",
    "df_tweets[\"index\"] = df_tweets.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines=[]\n",
    "for i in range(0,len(df_tweets)):\n",
    "    has= []\n",
    "    for j in range(0,len(df_tweets['extended_tweet'].values[i]['entities']['hashtags'])):\n",
    "        has.append(df_tweets['extended_tweet'].values[i]['entities']['hashtags'][j]['text'])\n",
    "        \n",
    "    lines.append(str(df_tweets[\"index\"].values[i]) +'|'+\n",
    "        str(df_tweets['extended_tweet'].values[i]['full_text'])+'|'+\n",
    "                  str(df_tweets['user'].values[i]['name']) +'|'+ \n",
    "                    str(df_tweets['created_at'].values[i]) +'|'+ str(has) +'|'\n",
    "                + str(df_tweets['favorite_count'].values[i])+'|'+str(df_tweets['retweet_count'].values[i])\n",
    "                 +'|'+ str(df_tweets['source'].values[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(s):  \n",
    "    \n",
    "    # initialize an empty string \n",
    "    str1 = \"\"  \n",
    "    \n",
    "    # traverse in the string   \n",
    "    for ele in s:  \n",
    "        str1 += ele   \n",
    "    \n",
    "    # return string   \n",
    "    return str1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(text):\n",
    "    if text:\n",
    "        # TODO: remove the accents from text (tip: search for unicode)\n",
    "        return unidecode.unidecode(text)\n",
    "        \n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def remove_punctuation_marks(text):\n",
    "    if text:\n",
    "        # TODO: Remove the punctuation marks from text (tip: search for translate and maketrans)\n",
    "        return text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def text_to_lower_case(text):\n",
    "    if text:\n",
    "        # TODO: convert text to lower case\n",
    "        return text.lower()\n",
    "    \n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def remove_emojis(text):\n",
    "    if text:\n",
    "        # TODO: Remove emojis (tip: search for encode - decode)\n",
    "        return text.encode('ascii','ignore').decode('ascii')\n",
    "\n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def remove_multiple_whitespaces(text):\n",
    "    if text:\n",
    "        # TODO: remove multiple whitespaces (tip: search for regex and replace)\n",
    "        return re.sub(\"\\s\\s+\", \" \", text)\n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "\n",
    "def remove_text_marks(text):\n",
    "    if text:\n",
    "        \n",
    "        #remove @+usernames\n",
    "        text =  \" \".join(filter(lambda x:x[0]!='@', text.split()))\n",
    "            \n",
    "        # TODO: replace characters like it\\'s by its\n",
    "        text = re.sub(r\"\\' \", \" \",text)\n",
    "        \n",
    "        # TODO: replace *, ?, ... by spaces\n",
    "        text = re.sub(r'[^\\w]', \" \",text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def split_text_and_numbers(text):\n",
    "    return text\n",
    "\n",
    "def remove_alone_numbers(text):\n",
    "    if text:\n",
    "        # TODO: keep only text\n",
    "        text = re.sub(r\"\\d\", \"\",text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def clean_text(text):\n",
    "    # Apply the different functions in order to clean the text\n",
    "    text = text_to_lower_case(text)\n",
    "    text = remove_text_marks(text)\n",
    "    text = remove_punctuation_marks(text)\n",
    "    text = remove_accents(text)\n",
    "    text = remove_emojis(text)\n",
    "    text = split_text_and_numbers(text)\n",
    "    text = remove_alone_numbers(text)\n",
    "    text = remove_multiple_whitespaces(text)\n",
    "    \n",
    "    # Return\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTerms(line):\n",
    "    \"\"\"\n",
    "    Preprocess the text removing stop words, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "    \n",
    "    Argument:\n",
    "    line -- string (text) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    line - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "        \n",
    "    stemming = PorterStemmer()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    ## START CODE\n",
    "    line = clean_text(line)\n",
    "    line= line.split(\" \") ## Tokenize the text to get a list of terms\n",
    "    \n",
    "    line=[x for x in line if x not in stops]  \n",
    "    \n",
    "    \n",
    "    ##eliminate the stopwords (HINT: use List Comprehension)\n",
    "    \n",
    "    line=[stemming.stem(word) for word in line] ## perform stemming (HINT: use List Comprehension)\n",
    "    \n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(lines):\n",
    "    \"\"\"\n",
    "    Impleent the inverted index\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of Wikipedia articles\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a python dictionary) containing terms as keys and the corresponding \n",
    "    list of document these keys appears in (and the positions) as values.\n",
    "    \"\"\"\n",
    "    index=defaultdict(list) \n",
    "    titleIndex = {} # dictionary to map page titles to page ids\n",
    "    for line in lines: # Remember, lines contain all documents, each line is a document\n",
    "        line_arr = line.split(\"|\")\n",
    "        page_id = int(line_arr[0] )\n",
    "        terms = getTerms(''.join(line_arr[1:])) #page_title + page_text\n",
    "        title = line_arr[1]            \n",
    "        titleIndex[page_id]=title  ## we do not need to apply get terms to title because it used only to print titles and not in the index\n",
    "        \n",
    "        ## ===============================================================        \n",
    "        ## create the index for the current doc and store it in termdictPage\n",
    "        ## termdictPage ==> { â€˜term1â€™: [currentdoc, [list of positions]], ...,â€˜termnâ€™: [currentdoc, [list of positions]]}\n",
    "        \n",
    "        ## Example: if the curr_doc has id 1 and his text is \n",
    "        ##Â \"web retrieval information retrieval\":\n",
    "        \n",
    "        ## termdictPage ==> { â€˜webâ€™: [1, [0]], â€˜retrievalâ€™: [1, [1,4]], â€˜informationâ€™: [1, [2]]}\n",
    "        \n",
    "        ## the term â€˜webâ€™ appears in document 1 in positions 0, \n",
    "        ## the term â€˜retrievalâ€™ appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "        \n",
    "        termdictPage={}\n",
    "\n",
    "        for position, term in enumerate(terms): # terms contains page_title + page_text. Loop over all terms\n",
    "            try:\n",
    "                # if the term is already in the index for the current page (termdictPage)\n",
    "                # append the position to the corrisponding list\n",
    "                \n",
    "        ## START CODE\n",
    "                termdictPage[term][1].append(position)  \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                termdictPage[term]=[page_id, array('I',[position])] #'I' indicates unsigned int (int in python)\n",
    "            \n",
    "        #merge the current page index with the main index\n",
    "        for termpage, postingpage in termdictPage.items():\n",
    "            index[termpage].append(postingpage)\n",
    "        \n",
    "        ## END CODE                    \n",
    "                    \n",
    "    return index, titleIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index, titleIndex = create_index(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Index results for the term 'stop': {}\\n\".format(index['stop']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = {}\n",
    "for i in titleIndex:\n",
    "    # remove \"RT\" string indicating a retweet\n",
    "    text = titleIndex[i].replace(\"RT\", \"\").strip()\n",
    "    \n",
    "    # lowering text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # removing all the punctuations\n",
    "    text = re.sub(r'[^\\w\\s]','',text).strip()\n",
    "    \n",
    "    # tokenize the text\n",
    "    lst_text = text.split()\n",
    "    \n",
    "    # remove stopwords\n",
    "    lst_text = [x for x in lst_text if x not in STOPWORDS]\n",
    "    \n",
    "        \n",
    "    # create bag-of-words - for each word the frequency of the word in the corpus\n",
    "    for w in lst_text:\n",
    "        if w not in bag_of_words:\n",
    "            bag_of_words[w] = 0\n",
    "        bag_of_words[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud(title, dic_):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(18,7))\n",
    "    wordcloud = WordCloud(background_color=\"white\",width=1600, height=800)\n",
    "    wordcloud = wordcloud.generate_from_frequencies(dic_)\n",
    "    ax.axis(\"off\")     \n",
    "    ax.imshow(wordcloud, interpolation='bilinear')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(top=0.8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcloud(\"WordCloud - All Tweets\", bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = dict(sorted(bag_of_words.items(), key=lambda item: item[1]))\n",
    "most_common_words= []\n",
    "values = []\n",
    "for i in range(1, 10):\n",
    "    most_common_words.append(list(bag_of_words)[-i])\n",
    "    values.append(bag_of_words[list(bag_of_words)[-i]])\n",
    "#most_common_words\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.bar(most_common_words,values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_tfidf(lines, numDocuments):\n",
    "    \"\"\"\n",
    "    Implement the inverted index and compute tf, df and idf\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of Wikipedia articles\n",
    "    numDocuments -- total number of documents\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a python dictionary) containing terms as keys and the corresponding \n",
    "    list of document these keys appears in (and the positions) as values.\n",
    "    tf - normalized term frequency for each term in each document\n",
    "    df - number of documents each term appear in\n",
    "    idf - inverse document frequency of each term\n",
    "    \"\"\"\n",
    "        \n",
    "    index=defaultdict(list)\n",
    "    tf=defaultdict(list) #term frequencies of terms in documents (documents in the same order as in the main index)\n",
    "    df=defaultdict(int)         #document frequencies of terms in the corpus\n",
    "    titleIndex=defaultdict(str)\n",
    "    idf=defaultdict(float)\n",
    "    \n",
    "    for line in lines:\n",
    "        line_arr = line.split(\"|\")\n",
    "        page_id = int(line_arr[0] )        \n",
    "        terms = getTerms(''.join(line_arr[1:])) #page_title + page_text\n",
    "        title = line_arr[1]            \n",
    "        titleIndex[page_id]=title          \n",
    "        \n",
    "        ## ===============================================================        \n",
    "        ## create the index for the **current page** and store it in termdictPage\n",
    "        ## termdictPage ==> { â€˜term1â€™: [currentdoc, [list of positions]], ...,â€˜termnâ€™: [currentdoc, [list of positions]]}\n",
    "        \n",
    "        ## Example: if the curr_doc has id 1 and his text is \n",
    "        ##Â \"web retrieval information retrieval\":\n",
    "        \n",
    "        ## termdictPage ==> { â€˜webâ€™: [1, [0]], â€˜retrievalâ€™: [1, [1,4]], â€˜informationâ€™: [1, [2]]}\n",
    "        \n",
    "        ## the term â€˜webâ€™ appears in document 1 in positions 0, \n",
    "        ## the term â€˜retrievalâ€™ appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "\n",
    "       \n",
    "        termdictPage={}\n",
    "\n",
    "        for position, term in enumerate(terms): ## terms contains page_title + page_text\n",
    "            try:\n",
    "                # if the term is already in the dict append the position to the corrisponding list\n",
    "                termdictPage[term][1].append(position) \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                termdictPage[term]=[page_id, array('I',[position])] #'I' indicates unsigned int (int in python)\n",
    "                \n",
    "        #normalize term frequencies\n",
    "        # Compute the denominator to normalize term frequencies (formula 2 above)\n",
    "        # norm is the same for all terms of a document.\n",
    "        norm=0\n",
    "        for term, posting in termdictPage.items(): \n",
    "            # posting is a list containing doc_id and the list of positions for current term in current document: \n",
    "            # posting ==> [currentdoc, [list of positions]] \n",
    "            # you can use it to inferr the frequency of current term.\n",
    "            norm+=len(posting[1])**2\n",
    "        norm=math.sqrt(norm)\n",
    "\n",
    "\n",
    "        #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in termdictPage.items():     \n",
    "            # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "            tf[term].append(np.round(len(posting[1])/norm,4))  ## SEE formula (1) above\n",
    "            #increment the document frequency of current term (number of documents containing the current term)\n",
    "            df[term]+= 1  # increment df for current term\n",
    "        \n",
    "        #merge the current page index with the main index\n",
    "        for termpage, postingpage in termdictPage.items():\n",
    "            index[termpage].append(postingpage)\n",
    "            \n",
    "        # Compute idf following the formula (3) above. HINT: use np.log\n",
    "        for term in df:\n",
    "            idf[term] = np.round(np.log(float(numDocuments/df[term])),4)\n",
    "            \n",
    "    return index, tf, df, idf, titleIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "numDocuments = len(lines)\n",
    "index, tf, df, idf, titleIndex = create_index_tfidf(lines, numDocuments)\n",
    "print(\"Total time to create the index: {} seconds\" .format(np.round(time.time() - start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Index results for the term 'stop': {}\\n\".format(index['stop']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranked documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF + cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rankDocuments(terms, docs, index, idf, tf, titleIndex):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    titleIndex -- mapping between page id and page title\n",
    "    \n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "        \n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
    "    # The remaing elements would became 0 when multiplied to the queryVector\n",
    "    docVectors=defaultdict(lambda: [0]*len(terms)) # I call docVectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
    "    queryVector=[0]*len(terms)    \n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms) # get the frequency of each term in the query. \n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})\n",
    "    #Â HINT: use when computing tf for queryVector\n",
    "    \n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "    \n",
    "    for termIndex, term in enumerate(terms): #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "                    \n",
    "        ##Â Compute tf*idf(normalize tf as done with documents)\n",
    "        queryVector[termIndex]=query_terms_count[term]/query_norm * idf[term] \n",
    "        \n",
    "        # Generate docVectors for matching docs\n",
    "        for docIndex, (doc, postings) in enumerate(index[term]):\n",
    "            # Example of [docIndex, (doc, postings)]\n",
    "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "            # term is in doc 26 in positions 1,4, .....\n",
    "            # term is in doc 33 in positions 26,33, .....\n",
    "            \n",
    "            #tf[term][0] will contain the tf of the term \"term\" in the doc 26            \n",
    "            if doc in docs:\n",
    "                docVectors[doc][termIndex]=tf[term][docIndex] * idf[term]  # TODO: check if multiply for idf\n",
    "\n",
    "    # calculate the score of each doc\n",
    "    # compute the cosine similarity between queyVector and each docVector:\n",
    "    # HINT: you can use the dot product because in case of normalized vectors it corresponds to the cosine siilarity\n",
    "    # see np.dot\n",
    "    \n",
    "    docScores=[ [np.dot(curDocVec, queryVector), doc] for doc, curDocVec in docVectors.items() ]\n",
    "    docScores.sort(reverse=True)\n",
    "    resultDocs=[x[1] for x in docScores]\n",
    "    #print document titles instead if document id's\n",
    "    #resultDocs=[ titleIndex[x] for x in resultDocs ]\n",
    "    if len(resultDocs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)    \n",
    "    #print ('\\n'.join(resultDocs), '\\n')\n",
    "    return resultDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tf_idf(query, index):\n",
    "    '''\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    '''\n",
    "    query=getTerms(query)\n",
    "    docs=set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in termDocs the ids of the docs that contain \"term\"                        \n",
    "            termDocs=[posting[0] for posting in index[term]]\n",
    "            \n",
    "            # docs = docs Union termDocs\n",
    "            docs |= set(termDocs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs=list(docs)\n",
    "    ranked_docs = rankDocuments(query, docs, index, idf, tf, titleIndex)   \n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Insert your query:\\n\")\n",
    "query = input()\n",
    "ranked_docs = search_tf_idf(query, index)    \n",
    "top = 20\n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the seached query:\\n\".format(top, len(ranked_docs)))\n",
    "for d_id in ranked_docs[:top] :\n",
    "    print(\"page_id= {} - page_title: {}\".format(d_id, titleIndex[d_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, index):\n",
    "    '''\n",
    "    The output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    '''\n",
    "    query=getTerms(query)\n",
    "    docs=set()\n",
    "    for term in query:\n",
    "    ## START DODE\n",
    "        try:\n",
    "            # store in termDocs the ids of the docs that contain \"term\"                        \n",
    "            termDocs=[posting[0] for posting in index[term]]\n",
    "            # docs = docs Union termDocs\n",
    "            docs |= set(termDocs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs=list(docs)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Insert your query:\\n\")\n",
    "query = input()\n",
    "docs = search(query, index)    \n",
    "top = 20\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the seached query:\\n\".format(top, len(docs)))\n",
    "for d_id in docs[:top] :\n",
    "    print(\"page_id= {} - page_title: {}\".format(d_id, titleIndex[d_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
